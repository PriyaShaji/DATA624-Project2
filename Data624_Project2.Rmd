---
title: "DATA 624 - Predictive Analytics"
subtitle: 'Fall 2020 - Project #2'
author: Vijaya Cherukuri, Samantha Deokinanan, Priya Shaji, Habib Khan
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    keep_tex: true
    df_print: kable
    toc: TRUE
    toc_depth: 3
urlcolor: purple
abstract: "An analysis of ABC Beverage to determine the pH level of the beverages and evaluate the accuracy of the predictive model with rigorous statistical testings."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.align="center")
```

### OVERVIEW

Due to new regulations by ABC Beverage, the company leadership requires that the production team have a better understanding of the manufacturing process, the predictive factors and their relationship to the pH of the beverages. Therefore, this project is an effort to find the optimal predictive variables related to the pH of the beverages and evaluate the accuracy of the predictive model for pH of beverages with rigorous statistical testing.

The selection of a method depends on many factors - the context of the predictions, the relevance of the historical data given, the degree of accuracy, etc. As a result, these factors are cross-validated and examined for the potentially greater accuracy model at minimal cost.

### R PACKAGES

The statistical tool that will be used to fascinate in the modeling of the data is `R`. The main packages used for data wrangling, visualization, and graphics are listed in the `Code Appendix`. Any other minor packages for analysis will be listed when needed.

```{r rpackages}
# Required R packages
library(tidyverse)
library(kableExtra)
library(psych)
library(caret)
library(mice)
library(corrplot)
library(xgboost)
library(Cubist)
library(randomForest)
```

### DATA EXPLORATION 

The data set is a historic data containing predictors associated to the pH and is provided in an excel file. We will utilize this historic data set to analyze and predict the pH of beverages. 

```{r loaddata}
# Set master seed
set.seed(52508)

# Set filepaths for data ingestion
urlRemote  = "https://github.com/"
pathGithub = "greeneyefirefly/DATA624-Project2/blob/main/"
fileTrain = "StudentData.xlsx"
fileTest = "StudentEvaluation.xlsx"

# Read training file
tempfile_1 = tempfile(fileext = ".xlsx")
tempfile_2 = tempfile(fileext = ".xlsx")

# Load training dataset
download.file(url = paste0(urlRemote, pathGithub, fileTrain, "?raw=true"), 
              destfile = tempfile_1, 
              mode = "wb", 
              quiet = TRUE)
train_df = data.frame(readxl::read_excel(tempfile_1,skip=0))

# Load test dataset
download.file(url =  paste0(urlRemote, pathGithub, fileTest, "?raw=true"), 
              destfile = tempfile_2, 
              mode = "wb", 
              quiet = TRUE)
eval_df = data.frame(readxl::read_excel(tempfile_2,skip=0))

# Number of training observations
ntrobs = dim(train_df)[[1]]

# Transform Brand.Code to factor
train_df$Brand.Code = as.factor(train_df$Brand.Code)
eval_df$Brand.Code = as.factor(eval_df$Brand.Code)
```

#### Predictive Variables

There are `r ntrobs` observations of 32 numeric predictor variables and 1 factor predictor variables, namely `Brand.Code`. All other variables provides measurements for the manufacturing process of each brand of beverage.

##### Summary Statistic

Based on the summary statistic for the beverage brands, we made some initial observations. The data set does not have complete cases, thus, there is a need for imputation. Some variables are highly skewed, such as `MFR`, `Temperature`, and `Oxygen.Filler`, which will need data transformation to satisfy the assumption of normality. Lastly, the `Hyd.Pressure` variables appear to be near-zero variance predictors given that zero account for more than 30% of these variables. Such zero variance predictor will never be chosen for a split since it offers no possible predictive information.

\small
```{r sumstat}
kable(describe(train_df)[,-c(1,6,7,13)], 
      caption = "Descriptive Statistics for All Brand Code",
      digit = 2L)
```
\normalsize

##### Missing Data

The graph below indicates the amount of missing data the training data contains. It appears that more than 8% of the missing data is from the `MFR` variable. This further suggest that 79% are complete. There are no missingness patterns, and their overall proportion is not extreme. This is good because for some imputation methods, such as certain types of multiple imputations, having fewer missingness patterns is helpful, as it requires fitting fewer models.

```{r missing}
na.counts = as.data.frame(((sapply(train_df, function(x) sum(is.na(x))))/nrow(train_df))*100)
names(na.counts) = "counts"
na.counts = cbind(variables = rownames(na.counts), data.frame(na.counts, row.names = NULL))

na.counts %>% arrange(counts) %>% mutate(name = factor(variables, levels = variables)) %>%
  ggplot(aes(x = name, y = counts)) + geom_segment( aes(xend = name, yend = 0)) +
  geom_point(size = 4, color = "steelblue2") + coord_flip() + theme_bw() +
  labs(title = "Proportion of Missing Data", x = "Variables", y = "% of Missing data") +
  scale_y_continuous(labels = scales::percent_format(scale = 1))
```

##### Outlier

Further exploration reveal that some variables may be strongly influenced by outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample. Outliers in data can distort predictions and affect the accuracy, therefore, these will need to be corrected by imputation.

```{r outliers}
temp = data.frame(variables = NA, outlier = NA)
for (i in 2:33){
  temp = rbind(temp, c(names(train_df)[i],
                             length(boxplot(train_df[i], plot = FALSE)$out)))
}
remove = apply(temp, 1, function(row) all(row !=0 )) 
temp = na.omit(temp[remove,])
row.names(temp) = NULL
kable(temp, caption = "Predictive Variables with Outlier")

ggplot(data = reshape2::melt(train_df) , aes(x = variable, y = value)) + 
geom_boxplot(fill = 'deeppink4') +
  labs(title = 'Boxplot: Scaled Training Set',
       x = 'Variables',
       y = 'Normalized Values')+
  theme(panel.background = element_rect(fill = 'grey'),
        axis.text.x = element_text(size = 10, angle = 90))
```

##### Correlation

The corrgram below graphically represents the correlations between the numeric predictor variables, when ignoring the missing variables. Most of the numeric variables are uncorrelated with one another, but there are a few highly correlated. 

```{r corrgram, fig.width=6, fig.height=6}
corrplot::corrplot(cor(train_df[,-1], use = 'complete.obs'),
         method = 'ellipse', type = 'lower', order = 'hclust',
         hclust.method = 'ward.D2', tl.cex = 0.7)
```

Moreover, to build a smaller model without predictors with extremely high correlations, it is best to reduce the number of predictors such that there are no absolute pairwise correlations above 0.90. The list below shows only significant correlations (at 5% level) for the top 10 highest correlations by the correlation coefficient. The results show that these ten have a correlation of greater than 0.95.

```{r corr}
corr = cor(train_df[,-1], use = 'complete.obs')
corr[corr == 1] = NA 
corr[abs(corr) < 0.90] = NA 
corr = na.omit(reshape::melt(corr))
kable(head(corr[order(-abs(corr$value)),], 10), 
      caption = "Top 10 Highly Correlated Predictor Candidates")
```

#### Target Variable (pH)

The response variable, `pH` has a couple of missing values. There is also a detection of outliers, which could explain the skewness in the variable. The plot below highlights that a majority of the pH level is less than 8.75.

```{r tragetviz}
train_df %>%
  ggplot(aes(PH, fill = PH > 8.75)) + 
  geom_histogram(bins = 30) +
  theme_bw() +
  theme(legend.position = 'center') +
  labs(y = 'Count', title = 'pH Levels in Dataset') 
```

### DATA PREPARATION

#### Pre-Processing of Predictors

Firstly, we will treat missing data and outlier by imputing them. Because it combines the standard linear regression and the nearest-neighbor imputation approach, we elected to fill in missing values of continuous variables by using the predictive mean matching imputation method. This will help to account for the uncertainty in the individual imputations. In addition, a near zero variable diagnoses is preformed on the predictors to identify if they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. If true, these will be removed.

```{r impute}
# Train set
processed_train_df = mice(train_df, method = 'pmm', print = FALSE)
train_df_cleaned = complete(processed_train_df)

predictors = nearZeroVar(train_df_cleaned)
train_df_cleaned = train_df_cleaned[,-predictors]

# Evaluation set
processed_eval_df = mice(eval_df, method = 'pmm', print = FALSE)
eval_df_cleaned = complete(processed_eval_df)

predictors = nearZeroVar(eval_df_cleaned)
eval_df_cleaned = eval_df_cleaned[,-predictors]
```

#### Correlation 

Next, to filter out highly correlated predictors, we remove those that have an absolute correlation coefficient greater than 0.90. This resulted in 25 of the 32 predictor variables are kept

```{r corr_clean}
# Train set
tooHigh = findCorrelation(cor(train_df_cleaned[,-1]), 0.90)
train_df_cleaned = train_df_cleaned[, -tooHigh]

# Evaluation set
tooHigh = findCorrelation(cor(eval_df_cleaned[,-1]), 0.90)
eval_df_cleaned = eval_df_cleaned[, -tooHigh]
```

#### Normality

The data is then pre-processed to fulfill the assumption of normality using the Yeo-Johnson transformation (Yeo and Johnson, 2000). This technique attempts to find the value of lambda that minimizes the Kullback-Leibler distance between the normal distribution and the transformed distribution. This method has the advantage of working without having to worry about the domain of x. 

```{r normality}
# Train set
processed_train_df = preProcess(train_df_cleaned[,-1], method = c("YeoJohnson"))
train_df_cleaned[,-1] =  predict(processed_train_df, train_df_cleaned[,-1])

# Evaluation set
processed_eval_df = preProcess(eval_df_cleaned[,-1], method = c("YeoJohnson"))
eval_df_cleaned[,-1] =  predict(processed_eval_df, eval_df_cleaned[,-1])
```

### Training & Testing Split

All the models will be trained on the same approximately 70% of the training set, reserving 30% for
validation of which model to select for the pH estimation on the supplied evaluation set.

```{r trainTestSplit}
# Create training and testing split from training data
set.seed(525)
intrain = createDataPartition(train_df_cleaned$PH, p = 0.70, list = FALSE)

# Train & Test predictor variables
train.p = train_df_cleaned[intrain, ] %>% select(-PH)
test.p = train_df_cleaned[-intrain, ] %>% select(-PH)

# Train & Test response variable (pH)
train.r = train_df_cleaned$PH[intrain]
test.r = train_df_cleaned$PH[-intrain]
```

### BUILDING THE MODELS

#### Model #1: Baseline Model

We will start with a simple linear model to serve as a baseline. This includes all variables in the data set. 

```{r baseline}
# Baseline linear model
baseline = lm(train.r ~ ., data = train.p)
kable(summary(baseline)$coefficients, digits = 3L,
      caption = 'Model 1 - Baseline Linear Regression Output')
```

We can immediately see that a few variables exceed the 0.05 p-value threshold for significance. The intercept itself suggests that with no information on the manufracting processes,a beverage is likely to have a pH balance of a killer 486! This baseline model also only account for 39.7% of the variability of the data. Clearly, there is a need for a better model.

#### Model #2: Multivariate Adaptive Regression Splines (MARS)

MARS is an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into non-linearity after grasping the concept of linear regression and other intrinsically linear models. Two tuning parameters associated with the MARS model is done to identify the optimal combination of these hyperparameters that minimize prediction error.

```{r mars}
set.seed(525) 
marsGrid = expand.grid(.degree = 1:2,
                       .nprune = 2:38) 
marsModel = train(x = train.p, 
                  y = train.r, 
                  method = "earth", 
                  tuneGrid = marsGrid, 
                  trControl = trainControl(method = "cv", 
                                           number = 10))
```
```{r mars.fig, fig.height=4, fig.width=8}
plot(marsModel, main = "RMSE of MARS Model")
```

RMSE was used to select the optimal MARS model using the smallest value. The best tune for the MARS model which resulted in the smallest root mean squared error is with 2 degrees of interactions and the number of retained terms of 31. It has RMSE = 0.126, and $R^2$ = 0.463. In this case, it does account for the largest portion of the variability in the data than all other variables, and it produces the smallest error. 

#### Model #3: Cubist

Cubist is a prediction-oriented regression model that initially creates a tree structure, and then collapses each path through the tree into a rule. A regression model is fit for each rule based on the data subset defined by the rules. The collection of rules are pruned or combined, and the candidate variables for the models are the predictors that were pruned away. 

```{r cubist}
set.seed(525)
cubModel = train(x = train.p,
                  y = train.r,
                  method = "cubist",
                  tuneLength = 10,
                  trControl = trainControl(method = "cv", 
                                           repeats = 5))
```
```{r cub.fig, fig.height=4, fig.width=8}
plot(cubModel, main = "RMSE of Cubist Tree Model")
```

RMSE was used to select the optimal model using the smallest value. The best tune for the cubist model which resulted in the smallest root mean squared error is with 20 committees and correct the prediction using the 9-nearest neighbors. It has RMSE = 0.117, and $R^2$ = 0.558. In this case, it does account for the largest portion of the variability in the data than all other variables, and it produces the smallest error which makes it the best fit.



#### Model #4: Partial Least Squares


```{r}
set.seed(393)
plsModel = train(x = train.p,
                  y = train.r,
                  method = "pls",
                  metric="rmse",
                 tuneLength = 20,
                 trControl= trainControl(method="cv"),
                 preProcess=c('center','scale'))
```
```{r cub.fig, fig.height=4, fig.width=8}
plot(plsModel, main = "RMSE of PLS Model")
```


#### Model #5: Gradient Boosting

```{r}
set.seed(20350)
grid <- expand.grid(n.trees=c(50, 100, 150, 200), 
                    interaction.depth=c(1, 5, 10, 15), 
                    shrinkage=c(0.01, 0.1, 0.5), 
                    n.minobsinnode=c(5, 10, 15))
gbm_m <- train(x = train.p,y = train.r, method = 'gbm',tuneGrid = grid, verbose = FALSE)

plot(gbm_m, main = "RMSE of Gradient Boosting Model")
```


#### Model #6: Support Vector Machine


```{r}
set.seed(9988)
rfModel <- train(x = train.p,y = train.r,
                  method='rf', tuneLength=4)

plot(rfModel, main = "RMSE of SVM Model")
```



### MODEL EVALUATION
#### Model Selection Criteria

In order to select the best model to make predictions, we looked at the following goodness of fit metrics:

 1. \(R^2\), which represents the proportion of the variance explained by the model
 2. *Root Mean Squared Error* (RMSE), which is the square root of the mean squared difference between the observation and the fitted value.
 3. *Mean Absolute Error* (MAE), which is the average of all absolute errors. Absolute Error is the amount of error in the measurements. It is the difference between the measured value and 'true' value.

We will look at \(R^2\) and RMSE as our metrics and select the model which performs best on both. If no model performs best on both, the MAE of the model results on the training set will be used to break the tie. The remaining 20% of the historical data (i.e. predictors `test.p` and response `test.r`) will be used to determine the model performance.

The table below highlights the performance criteria of all the models built. It is evident that Random Forest is the best model as it has the lowest RMSE value compared to all other models.


```{r compTable}
set.seed(525)
marsPred = predict(marsModel, newdata = test.p)
cubPred = predict(cubModel , newdata = test.p)
plsPred = predict(plsModel, newdata= test.p)
gradPred = predict(gbm_m, newdata = test.p)
rfPred = predict(rfModel, newdata= test.p)

compTable = data.frame(rbind(MARS = postResample(pred = marsPred, obs = test.r),
                             CUBIST = postResample(pred = cubPred, obs = test.r),
                             PLS = postResample(pred = plsPred, obs= test.r),
                             GRADIENT_BOOSTING = postResample(pred= gradPred, obs=test.r),
                             Random_Forest = postResample(pred=rfPred, obs = test.r)))

kable(compTable, caption = "Performance Metric for All Models")
```

#### Variable Importances

Let's display informative variables picked by Random Forest model using `varImp` since it has best fit and also lowest RMSE.


```{r}
varImp(rfModel)
```


Random Forest model picked the most informative variables among which top three variables are `Mnf.Flow`, `Brand.Code` and `Usage.cont`. These are the variables which are most related to the pH of beverages.

Variable importance plot 

```{r}
dotPlot(varImp(rfModel), top=10)
```


Therefore, `Mnf.Flow` is the most important variable followed by `Brand.Code` , affecting pH value of beverages.


### MODEL SELECTION


### PREDICTION RESULT

Based on the performance test, it was decided that the `BEST MODEL HERE` model is the optimal model. With the test set data processed identically to the training set, it will be used to make the pH prediction of beverages given the evaluation data. Since the number of cases needs to be integral the predicted values will be rounded to the nearest integer.


#### Save pH Predictions

eval_df_cleaned is the cleaned evaluation set

### CONCLUSION

### WORKS CITED

 1. Yeo, I., & Johnson, R. (2000). A New Family of Power Transformations to Improve Normality or Symmetry. Biometrika, 87(4), 954â€“959. Retrieved November 26, 2020, from http://www.jstor.org/stable/2673623

 2. Max Kuhn and Kjell Johnson. Applied Predictive Modeling. Springer, New York, 2013.
 
 3.

### CODE APPENDIX

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r rpackages}
```
```{r loaddata}
```
```{r sumstat}
```
```{r missing}
```
```{r outliers}
```
```{r corrgram, fig.width=7, fig.height=7}
```
```{r corr}
```
```{r tragetviz}
```
```{r impute}
```
```{r corr_clean}
```
```{r normality}
```
```{r trainTestSplit}
```
```{r baseline}
```
```{r mars}
```
```{r mars.fig, fig.height=4, fig.width=8}
```
```{r cubist}
```
```{r cub.fig, fig.height=4, fig.width=8}
```
```{r compTable}
```


<--- ADD NAME OF CODE CHUNK HERE TO PRINT AT THE END --->
